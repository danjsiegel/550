{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "import timeit\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous lesson, you created a corpus preprocessor to process the featured articles from Wikipedia. In this exercise, you will re-implement the preprocessor to use Python’s multiprocessing library.\n",
    "\n",
    "Throughout this exercise, you will use Python’s timeit library to measure the performance of certain parts of your code. Implement your code so you can specify the number of processes your code is allowed to use. Report your performance results for N=1, 2, 4, 8, and 16 processes using the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wiki_articles:  \n",
    "    def __init__(self, path, filetype, njobs):\n",
    "        self.json_files = []\n",
    "        self.all_articles = []\n",
    "        self.njobs = njobs\n",
    "        self.get_articles(path, filetype)\n",
    "        self.read_entire_directory()\n",
    "    def get_articles(self, path, filetype):\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(filetype):\n",
    "                file_path = path + file\n",
    "                self.json_files.append(file_path)\n",
    "    def read_articles(self, article_file):\n",
    "        with open(article_file) as f:\n",
    "            lines = f.readlines()\n",
    "        articles = [json.loads(line) for line in lines]\n",
    "        self.all_articles.append(articles)\n",
    "    def read_entire_directory(self):\n",
    "        with Pool(self.njobs) as pool:\n",
    "            pool.map(self.read_articles, self.json_files)\n",
    "    def ret_articles(self):\n",
    "        return self.all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [1,2,4,8,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs:\n",
    "    %time reading_articles = wiki_articles(\"C:\\\\Users\\\\Dan Siegel\\\\Desktop\\\\Classes\\\\550\\\\data\\\\wikipedia\\\\featured-articles\\\\\", \"jsonl\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Tokenize articles\n",
    "\n",
    "Create a function or class method that tokenizes each of the articles from the previous step. Measure the time it takes to tokenize all of the articles for the different number of processes.\n",
    "\n",
    "The following provides a rough outline of the code for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wiki_articles:  \n",
    "    def __init__(self, path, filetype, njobs):\n",
    "        self.json_files = []\n",
    "        self.all_articles = []\n",
    "        self.njobs = njobs\n",
    "        self.get_articles(path, filetype)\n",
    "    def get_articles(self, path, filetype):\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(filetype):\n",
    "                file_path = path + file\n",
    "                self.json_files.append(file_path)\n",
    "    def read_articles(self):\n",
    "        for i in self.json_files:\n",
    "            with open(i) as f:\n",
    "                lines = f.readlines()\n",
    "            articles = [json.loads(line) for line in lines]\n",
    "            self.all_articles.append(articles)\n",
    "    def tokenize_document(self, document):\n",
    "        toks = nltk.word_tokenize(document['section_texts'])\n",
    "        return toks\n",
    "    def tokenize_documents(self):\n",
    "        with Pool(njobs) as pool:\n",
    "            tokenized_docs = pool.map(self.tokenize_document, self.all_articles)\n",
    "    def ret_articles(self):\n",
    "        return self.all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs:\n",
    "    reading_articles = wiki_articles(\"C:\\\\Users\\\\Dan Siegel\\\\Desktop\\\\Classes\\\\550\\\\data\\\\wikipedia\\\\featured-articles\\\\\", \"jsonl\", i)\n",
    "    %time reading_articles.tokenize_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Word Count\n",
    "\n",
    "Create a function or a class method that counts the number of times each word appears throughout the entire collection of articles. The result should be a Python dictionary containing each word as a key and the word count as a value.\n",
    "\n",
    "You should use the map-reduce paradigm when implementing your code. The previous two steps demonstrated the use of the map function. The map function simply applies a function across a list of inputs. The map function is naturally scalable as each is the result of applying the function to each input is independent. In a cluster compute environment like Hadoop, the map function is used to distribute tasks across multiple work nodes. Each map task processes data that is local to the computer it is working on and there is no need to share information between the worker nodes.\n",
    "\n",
    "The reduce function combines the results from all the different map tasks into a single result. The output of each map task is a collection of key/value pairs (in our case a Python dictionary where the key is the word and the value is the word count). The reduce part of the algorithm applies a function to task results that share the same key. For this exercise, that function is the sum function that adds all of the word counts together.\n",
    "\n",
    "While the details of how Hadoop performs parallel computations is beyond the scope of this exercise, the map-reduce paradigm should give you a general idea of how tasks are distributed across multiple machines.\n",
    "\n",
    "The following code provides a very rough outline of how to implement word counting using the map-reduce paradigm. You should ensure the map portion of the code runs using multiple processes. The reduce portion will not use multiple processes. Perform the word count task using the different number of processes and report the performance results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "class wiki_articles:  \n",
    "    def __init__(self, path, filetype, njobs):\n",
    "        self.json_files = []\n",
    "        self.all_articles = []\n",
    "        self.njobs = njobs\n",
    "        self.get_articles(path, filetype)\n",
    "    def get_articles(self, path, filetype):\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(filetype):\n",
    "                file_path = path + file\n",
    "                self.json_files.append(file_path)\n",
    "    def read_articles(self):\n",
    "        for i in self.json_files:\n",
    "            with open(i) as f:\n",
    "                lines = f.readlines()\n",
    "            articles = [json.loads(line) for line in lines]\n",
    "            self.all_articles.append(articles)\n",
    "    def tokenize_document(self, document):\n",
    "        toks = nltk.word_tokenize(document['section_texts'])\n",
    "        return toks\n",
    "    def tokenize_documents(self):\n",
    "        with Pool(njobs) as pool:\n",
    "            tokenized_docs = pool.map(self.tokenize_document, self.all_articles)\n",
    "    def ret_articles(self):\n",
    "        return self.all_articles\n",
    "    def count_words(self, document):\n",
    "        data = dict(Counter(document['section_texts'].split()))\n",
    "        return data\n",
    "    def count_all_words(self):\n",
    "        word_count_dics = map(self.count_words, self.all_articles)\n",
    "        word_counts = reduc(dict_sum, word_count_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs:\n",
    "    reading_articles = wiki_articles(\"C:\\\\Users\\\\Dan Siegel\\\\Desktop\\\\Classes\\\\550\\\\data\\\\wikipedia\\\\featured-articles\\\\\", \"jsonl\", i)\n",
    "    %time reading_articles.count_all_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Workflow\n",
    "Create workflows and the associated tasks for the following:\n",
    "- Text preprocessing\n",
    "- Text classification\n",
    "    - Binary text classification, model evaluation, and selection\n",
    "    - Multi-class text classification, model evaluation, and selection\n",
    "- Topic modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing:\n",
    "def task1(param1,param2):\n",
    "    ''' ingest and find all the input file(s) that will be used in the preprocessing\n",
    "    Args:\n",
    "        Param1(str): File Path\n",
    "        Param2(str): File Extension\n",
    "    Returns:\n",
    "        List:  all files and their full paths to be read into the workflow\n",
    "    '''\n",
    "def task2(param2,param3):\n",
    "    ''' takes the files identified by task1 and reads them into a list based on their extension type\n",
    "    Args:\n",
    "        Param2(str): File Extension\n",
    "        Param3(list): Full Filepaths of all expected files\n",
    "    Returns:\n",
    "        List: all files read into their own list index  \n",
    "    '''\n",
    "def task3(param4):\n",
    "    '''Takes the raw text read which was read into a list and removes punctuation, \n",
    "    stopwords, stemms the words \n",
    "    Args:\n",
    "        Param4(list): list of all unprocessed text, each document representing it's own index\n",
    "    Returns:\n",
    "        List: List of stemmed words without punctuation or stopwords. \n",
    "    '''\n",
    "#Text classification\n",
    "def task4(param5, param6, param7):\n",
    "    '''Takes the cleaned text and runs it through classification models\n",
    "    Args:\n",
    "        Param5(list): stemmed words without punctuation or stopwords\n",
    "        Param6(list): list of models to apply to the corpus\n",
    "        Param7(binary): if this is a binary classification yes or no\n",
    "    Returns:\n",
    "        List: List of stemmed words without punctuation or stopwords and models trained on the data and tested.\n",
    "        Matrix: returns a confusion matrix with scoring\n",
    "    '''\n",
    "#topic modeling\n",
    "def task5(param5):\n",
    "    '''Takes the cleaned text and runs it through topic modeling models\n",
    "    Args:\n",
    "        Param5(list): stemmed words without punctuation or stopwords\n",
    "    Returns:\n",
    "        List: returns predictions of topics \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = {\n",
    "    \"tasks\": [\n",
    "        (task1, task2),\n",
    "        (task1, task3),\n",
    "        (task1, task5),\n",
    "        (task2, task5),\n",
    "        (task3, task5),\n",
    "        (task2, task4),\n",
    "        (task3, task4)\n",
    "    ],\n",
    "    \"params\": {\n",
    "        \"param\": \"File Path\",\n",
    "        \"param\": \"File Extension\",\n",
    "        \"param\": \"Full Filepaths of all expected files\",\n",
    "        \"param\": \"list of all unprocessed text, each document representing it's own index\",\n",
    "        \"param\": \"stemmed words without punctuation or stopwords\",\n",
    "        \"param\": \"list of models to apply to the corpus\",\n",
    "        \"param\": \"if this is a binary classification yes or no\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
