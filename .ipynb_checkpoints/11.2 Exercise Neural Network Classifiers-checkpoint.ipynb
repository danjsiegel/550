{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Classifier with Scikit\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the\n",
    "Applied Text Analysis with Python\n",
    "book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'C:\\\\Users\\\\Dan Siegel\\\\Desktop\\\\Classes\\\\550\\\\data\\\\reddit\\\\categorized-comments.jsonl'\n",
    "data = []\n",
    "with open(json_path) as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))        \n",
    "df = pd.DataFrame.from_dict(json_normalize(data), orient='columns')\n",
    "df = df.sample(frac=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1789214</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>&amp;gt; You make maintaining a really short how-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299605</th>\n",
       "      <td>video_games</td>\n",
       "      <td>I'm a 760(? Went broke shortly after emerald n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093467</th>\n",
       "      <td>video_games</td>\n",
       "      <td>yes ppl think its more fun if they can play wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672471</th>\n",
       "      <td>video_games</td>\n",
       "      <td>USE YOUR REINFORCEMENTS FFS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361454</th>\n",
       "      <td>sports</td>\n",
       "      <td>wonder whos big spoon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            cat                                                txt\n",
       "1789214  science_and_technology  &gt; You make maintaining a really short how-t...\n",
       "2299605             video_games  I'm a 760(? Went broke shortly after emerald n...\n",
       "1093467             video_games  yes ppl think its more fun if they can play wh...\n",
       "672471              video_games                        USE YOUR REINFORCEMENTS FFS\n",
       "1361454                  sports                              wonder whos big spoon"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(df.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categorical_cat'] = df['cat'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, df.categorical_cat, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(500,150), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.90891358\n",
      "Iteration 2, loss = 0.48719700\n",
      "Iteration 3, loss = 0.31482848\n",
      "Iteration 4, loss = 0.23548926\n",
      "Iteration 5, loss = 0.18821232\n",
      "Iteration 6, loss = 0.15467965\n",
      "Iteration 7, loss = 0.13621932\n",
      "Iteration 8, loss = 0.12646041\n",
      "Iteration 9, loss = 0.12243083\n",
      "Iteration 10, loss = 0.12034677\n",
      "Iteration 11, loss = 0.11832514\n",
      "Iteration 12, loss = 0.11715692\n",
      "Iteration 13, loss = 0.11586256\n",
      "Iteration 14, loss = 0.11531876\n",
      "Iteration 15, loss = 0.11447640\n",
      "Iteration 16, loss = 0.11393003\n",
      "Iteration 17, loss = 0.11421544\n",
      "Iteration 18, loss = 0.11438200\n",
      "Iteration 19, loss = 0.11380901\n",
      "Iteration 20, loss = 0.11283838\n",
      "Iteration 21, loss = 0.11304803\n",
      "Iteration 22, loss = 0.11236755\n",
      "Iteration 23, loss = 0.11198006\n",
      "Iteration 24, loss = 0.11189459\n",
      "Iteration 25, loss = 0.11186551\n",
      "Iteration 26, loss = 0.11182081\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(500, 150), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_func(predictions, test_scores):\n",
    "    accu_score = accuracy_score(test_scores, predictions)\n",
    "    precision = precision_score(test_scores, predictions, average='macro')\n",
    "    recall = recall_score(test_scores, predictions, average='macro')\n",
    "    f1 = f1_score(test_scores, predictions, average='macro')\n",
    "    conf=confusion_matrix(test_scores, predictions)\n",
    "    print ('accuracy: ',accu_score)\n",
    "    print('precision:', precision)\n",
    "    print ('Recall:', recall)\n",
    "    print('F1:', f1)\n",
    "    print('confusion matrix:', conf)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6655479540467277\n",
      "precision: 0.5951752808178006\n",
      "Recall: 0.5705080922531287\n",
      "F1: 0.5787725055491136\n",
      "confusion matrix: [[1565  147  499  471]\n",
      " [ 324  276  157  303]\n",
      " [ 359   76 3606 1069]\n",
      " [ 407  184 1186 4865]]\n"
     ]
    }
   ],
   "source": [
    "scoring_func(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network Classifier with Keras\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the\n",
    "Applied Text Analysis with Python\n",
    "book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(): \n",
    "    \"\"\" \n",
    "    Create a function that returns a compiled neural network \n",
    "    \"\"\" \n",
    "    nn = Sequential() \n",
    "    nn.add(Dense(500, activation = 'relu', input_shape =(46770,))) \n",
    "    nn.add(Dense(150, activation = 'relu')) \n",
    "    nn.add(Dense(4, activation = 'softmax')) \n",
    "    nn.compile(loss ='categorical_crossentropy', \n",
    "               optimizer ='adam', \n",
    "               metrics =['accuracy']) \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "KC = KerasClassifier(build_fn=build_network, nb_epoch=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31456/31456 [==============================] - 363s 12ms/step - loss: 0.8203 - acc: 0.6583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bf5186b4e0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_ypred = KC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6969794759261649\n",
      "precision: 0.6476370498590679\n",
      "Recall: 0.6108644877271785\n",
      "F1: 0.6236304925419752\n",
      "confusion matrix: [[1410  278  576  418]\n",
      " [ 158  411  155  336]\n",
      " [ 182   68 3950  910]\n",
      " [ 188  162 1264 5028]]\n"
     ]
    }
   ],
   "source": [
    "scoring_func(kc_ypred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifying Images\n",
    "\n",
    "In chapter 20 of the\n",
    "Machine Learning with Python Cookbook\n",
    ", implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(_):\n",
    "    return _.reshape(_.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = reshape_data(data_train)\n",
    "data_test = reshape_data(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = data_train / 255 \n",
    "features_test = data_test / 255\n",
    "target_train = np_utils.to_categorical(target_train) \n",
    "target_test = np_utils.to_categorical(target_test) \n",
    "number_of_classes = target_test.shape[1]\n",
    "network = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.5803 - acc: 0.8197\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 198s 3ms/step - loss: 0.1869 - acc: 0.9447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bf53d11278>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.add(Conv2D(filters = 64, kernel_size=(5,5), input_shape=(1, 28, 28), activation ='relu'))\n",
    "network.add(MaxPooling2D(pool_size =(2, 2)))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(128, activation =\"relu\")) \n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(number_of_classes, activation =\"softmax\"))\n",
    "network.compile(loss =\"categorical_crossentropy\", optimizer =\"rmsprop\", metrics =[\"accuracy\"])\n",
    "network.fit(features_train, target_train, epochs = 2, batch_size = 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_network = network.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9643"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, conv_network.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
