{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bellevue.edu Site Link Analysis\n",
    "\n",
    "In this part of the exercise, you will be examining the Bellevue University web site. The bellevue.edu directory contains two CSV files, all_links.csv and content_links.csv. The all_links.csv contains all the links between the pages including links from the default navigation bar. The content_links.csv file only contains links between pages if those links occurred in the content of the page and not in the navigation bar.\n",
    "\n",
    "The files contain two columns of data, src_link and dst_link. The src_link indicates the page that contains a link to the dst_link. The links are relative to the main page, so / resolves to http://www.bellevue.edu while /degrees resolves to http://www.bellevue.edu/degrees.\n",
    "\n",
    "For this exercise, you will need to create a Networkx DiGraph from both datasets. A Networkx DiGraph is a directed graph where each node is a relative URL for a page on the Bellevue University website, and each edge represents a link from one page to another. See Networkxâ€™s tutorial if you need help creating these graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_data_root = 'C:\\\\Users\\\\Dan Siegel\\\\Desktop\\\\Classes\\\\550\\\\data\\\\bellevue.edu\\\\'\n",
    "all_links_path = bv_data_root+'all_links.csv'\n",
    "content_links_path = bv_data_root+'content_links.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_df = pd.read_csv(all_links_path)\n",
    "content_links_df = pd.read_csv(content_links_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "from nltk import sent_tokenize \n",
    "def cooccurrence(text, cast): \n",
    "    \"\"\" Takes as input text, a dict of chapter {headings: text}, and cast, a comma separated list of character names. \n",
    "    Returns a dictionary of cooccurrence counts for each possible pair. \"\"\" \n",
    "    possible_pairs = list( itertools.combinations( cast, 2)) \n",
    "    cooccurring = dict.fromkeys(possible_pairs, 0) \n",
    "    for title, chapter in text[' chapters']. items(): \n",
    "        for sent in sent_tokenize( chapter): \n",
    "            for pair in possible_pairs: \n",
    "                if pair[ 0] in sent and pair[ 1] in sent: \n",
    "                    cooccurring[ pair] += 1 \n",
    "                    return cooccurring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(all_links_df, source='src_link', target='dst_link', edge_attr=True, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dict(nx.shortest_path_length(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest = pd.DataFrame.from_dict(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest['/'] = shortest['/'].fillna(value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Clicks from the Homepage\n",
    "\n",
    "Using the all_links.csv dataset, you will determine how many clicks it takes to get from the homepage to every other page. You can calculate the number of clicks it takes to get from the homepage to any other page by calculating the shortest path length from the homepage (/) as the source node and the destination link as the target node. You may need to check if there is a path between the homepage and the destination link.\n",
    "\n",
    "Provide a summary of the number of clicks each page is away from the main page (i.e., the / link) using the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 2.0    423\n",
       " 3.0    273\n",
       " 1.0     74\n",
       "-1.0     13\n",
       " 4.0      2\n",
       " 0.0      1\n",
       "Name: /, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest['/'].value_counts()\n",
    "#-1 is No path links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. PageRank\n",
    "\n",
    "Using the content_links.csv data, calculate the PageRank for each node. PageRank is an algorithm first developed by Google to determine the importance of a website by looking at the links between websites. Intuitively, the PageRank algorithm simulates a web user clicking random links on a website. The PageRank of a site is a measure of the likelihood a user would land on that site by randomly clicking links.\n",
    "\n",
    "This means a page is more important if other pages link to it.\n",
    "\n",
    "Provide a summary of the top 20 pages by PageRank using the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "GG = nx.from_pandas_edgelist(content_links_df, source='src_link', target='dst_link', edge_attr=True, create_using=nx.DiGraph())\n",
    "pr = nx.pagerank(GG, alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_rank_df = pd.DataFrame.from_dict(pr, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.sort_values of /about/about-us/community-service                     0.002317\n",
       "/about/about-us                                       0.002285\n",
       "/about/about-us/history                               0.002285\n",
       "/about/about-us/locations                             0.004939\n",
       "/about/about-us/mission-values                        0.002285\n",
       "/about/about-us/news-events-calendar/calendar         0.002520\n",
       "/about/about-us/pratt-award                           0.002359\n",
       "/about/about-us/remembering-our-fallen                0.002285\n",
       "/about/about-us/statistics-facts                      0.002397\n",
       "/about/about-us/student-stories                       0.002285\n",
       "/about/gcc                                            0.002496\n",
       "/degrees/academic-catalog/signature-series            0.018014\n",
       "/about/about-us/latino-dream                          0.000337\n",
       "/about/about-us/community-service/latino-dream        0.000163\n",
       "/admissions-tuition/financing-options/how-to-apply    0.002650\n",
       "/about/about-us/locations/area-map                    0.000879\n",
       "/about/about-us/locations/campus-map                  0.000709\n",
       "/about/about-us/locations/index.aspx                  0.000167\n",
       "/about/about-us/area-map                              0.000534\n",
       "/about/about-us/campus-map                            0.000534\n",
       "Name: 0, dtype: float64>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_rank_df[0].head(20).sort_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Featured Articles Entity Graph\n",
    "\n",
    "In this part of the exercise, you will extract entity pairs from the Wikipedia featured articles data set. Extract the entity pairs and load them into a Networkx graph. Using this graph, report the following basic information.\n",
    "\n",
    "reporting format\n",
    "\n",
    "a. Report on the top 20 nodes for each rank for degree centrality\n",
    "\n",
    "Report the top 20 degree centrality rankings as described in chapter 9 of the Applied Text Analysis with Python book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Dan Siegel\\\\Desktop\\\\Classes\\\\550\\\\data\\\\wikipedia\\\\featured-articles\\\\featured-articles_000.jsonl'\n",
    "with open(path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "articles = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.DataFrame.from_records(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df.interlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggg = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in articles_df:\n",
    "    list_of_dicts.append(i)\n",
    "for i in list_of_dicts:\n",
    "    ggg.add_nodes_from(i.keys())\n",
    "for i in list_of_dicts:\n",
    "    for k, v in i.items():\n",
    "        ggg.add_edges_from(([(k, t) for t in v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(ggg, alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_rank_df = pd.DataFrame.from_dict(pr, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.sort_values of                                    0\n",
       "developmental disorder      0.000009\n",
       "Interpersonal relationship  0.000009\n",
       "communication               0.000009\n",
       "behavior                    0.000009\n",
       "developmental milestones    0.000009\n",
       "Regressive autism           0.000009\n",
       "Heritability of autism      0.000009\n",
       "environmental factors       0.000009\n",
       "infection                   0.000009\n",
       "pregnancy                   0.000009\n",
       "rubella                     0.000009\n",
       "valproic acid               0.000009\n",
       "Alcohol (drug)              0.000009\n",
       "cocaine                     0.000009\n",
       "Controversies in autism     0.000009\n",
       "Causes of autism            0.000009\n",
       "MMR vaccine controversy     0.000009\n",
       "brain                       0.000009\n",
       "nerve cell                  0.000009\n",
       "synapse                     0.000009>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centrality_rank_df.head(20).sort_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Report on the top 20 nodes for each rank for betweenness centrality Report the top 20 betweenness centrality rankings as described in chapter 9 of the Applied Text Analysis with Python book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.sort_values of                                    0\n",
       "developmental disorder      0.000009\n",
       "Interpersonal relationship  0.000009\n",
       "communication               0.000009\n",
       "behavior                    0.000009\n",
       "developmental milestones    0.000009\n",
       "Regressive autism           0.000009\n",
       "Heritability of autism      0.000009\n",
       "environmental factors       0.000009\n",
       "infection                   0.000009\n",
       "pregnancy                   0.000009\n",
       "rubella                     0.000009\n",
       "valproic acid               0.000009\n",
       "Alcohol (drug)              0.000009\n",
       "cocaine                     0.000009\n",
       "Controversies in autism     0.000009\n",
       "Causes of autism            0.000009\n",
       "MMR vaccine controversy     0.000009\n",
       "brain                       0.000009\n",
       "nerve cell                  0.000009\n",
       "synapse                     0.000009>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centrality_rank_df.head(20).sort_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Bot Detection on Reddit\n",
    "\n",
    "Your data directory should contain the file user_comment_links.csv the reddit directory. This file contains three fields.\n",
    "\n",
    "user1: The Reddit username of the first user\n",
    "user2: The Reddit username of the second user\n",
    "num_comments: The number of times user1 commented on something user2 wrote\n",
    "Using Networkx, create a directed graph of user comments. The following is Python demonstrating how to add a weighted edge in a directed graph.\n",
    "\n",
    "reporting format\n",
    "\n",
    "In this exercise, you will use the degree centrality algorithm to find users who a central to in the communication graph. These users are most likely automated bots which explain how they are able to communicate with a wide variety of users across multiple subreddits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "wel = pd.read_csv('C:\\\\Users\\\\Dan Siegel\\\\Desktop\\\\Classes\\\\550\\\\data\\\\reddit\\\\user_comment_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "redG = nx.DiGraph()\n",
    "redG.add_weighted_edges_from([tuple(x) for x in wel.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df for average degrees\n",
    "dsdff = pd.DataFrame.from_dict(dict(redG.degree), orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df for avg out degrees\n",
    "redgOutDeg = pd.DataFrame.from_dict(dict(redG.out_degree), orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Graph Information\n",
    "\n",
    "Report the following basic information.\n",
    "Nodes, edges, Average Degree, Average out degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160746, 1724191, 0    2.970833\n",
       " dtype: float64, 0    1.485416\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redG.nodes), len(redG.edges), dsdff.mean(), redgOutDeg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reddit_centrality = pd.DataFrame.from_dict(nx.degree_centrality(redG), orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Degree Centrality\n",
    "\n",
    "Report the top 20 degree centrality rankings as described in chapter 9 of the Applied Text Analysis with Python book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.sort_values of                                  0\n",
       "-----------------www  8.615157e-07\n",
       "zcc0nonA              2.584547e-06\n",
       "----------_----       3.015305e-05\n",
       "Amicus-Regis          3.618366e-05\n",
       "BobIV                 6.202913e-05\n",
       "Dont-worry-about-it   3.446063e-06\n",
       "Felikitsune           2.239941e-05\n",
       "HeyImNiko             2.756850e-05\n",
       "Iguessimnotcreative   1.550728e-05\n",
       "KokuTatsu             5.599852e-05\n",
       "Last-Man-Standing     9.476672e-06\n",
       "Mimighster            1.723031e-06\n",
       "Saskatchemoose        5.169094e-06\n",
       "Swiffles33            8.615157e-07\n",
       "TheRisingDownfall     9.476672e-06\n",
       "TheSamshinCashew      8.615157e-07\n",
       "UnpleasantVisitor     3.446063e-06\n",
       "bmierror              6.892125e-06\n",
       "ininja2               3.446063e-06\n",
       "itztaytay             3.962972e-05>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reddit_centrality.head(20).sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
