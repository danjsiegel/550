{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Dan Siegel\\\\Desktop\\\\Classes\\\\550\\\\data\\\\wikipedia\\\\featured-articles\\\\featured-articles_000.jsonl'\n",
    "with open(path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "articles = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Dan\n",
      "[nltk_data]     Siegel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a Corpus\n",
    "\n",
    "Create a summary of the corpus with the following information.\n",
    "* Documents - Number of documents in the corpus.\n",
    "* Paragraphs - Number of paragraphs in the corpus.\n",
    "* Sentences - Number of sentences in the corpus.\n",
    "* Words - Total number of words in the corpus.\n",
    "* Unique Words - Total number of unique words (i.e., size of the corpus vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_text = []\n",
    "i=0\n",
    "for article in articles:\n",
    "    section_text.append(articles[i]['section_texts'])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = 0\n",
    "for sections in range(len(section_text)):\n",
    "    for paras in range(len(section_text[sections])):\n",
    "        section_text[sections][paras] = section_text[sections][paras].rstrip().lstrip()\n",
    "        counts = counts + section_text[sections][paras].count('\\n')\n",
    "        section_text[sections][paras] = section_text[sections][paras].strip('\\n').strip('\\n*').strip('* \\n*').strip(' ').strip('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "flattened_list  = list(itertools.chain(section_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Documents - Number of documents in the corpus.\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3062"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Paragraphs - Number of paragraphs in the corpus.\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88325\n"
     ]
    }
   ],
   "source": [
    "#Sentences - Number of sentences in the corpus\n",
    "extra_copy = [item for sublist in flattened_list for item in sublist]\n",
    "sent = []\n",
    "for article in range(len(extra_copy)):\n",
    "    sent.append(nltk.sent_tokenize(str(extra_copy[article])))\n",
    "flattened_sent  = [item for sublist in sent for item in sublist]\n",
    "print(len(flattened_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2313470\n"
     ]
    }
   ],
   "source": [
    "#Words - Total number of words in the corpus.\n",
    "extra_copy = [item for sublist in flattened_list for item in sublist]\n",
    "word_corp = []\n",
    "for article in range(len(extra_copy)):\n",
    "    word_corp.append(nltk.word_tokenize(str(extra_copy[article])))\n",
    "flattened_word  = [item for sublist in word_corp for item in sublist]\n",
    "print(len(flattened_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words 103093\n"
     ]
    }
   ],
   "source": [
    "#Unique Words - Total number of unique words (i.e., size of the corpus vocabulary)\n",
    "np_flat = np.array(flattened_word)\n",
    "print('unique words', len(np.unique(np_flat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Word Count Matrix \n",
    "Using the corpus from the previous problem, create a matrix of word counts using Numpy arrays. The output matrix should be of dimensions  N x M where N is the number of documents, and M is the number of words in the corpus vocabulary.\n",
    "\n",
    "You may notice the size of this matrix increases rapidly for a large number of documents or a large corpus vocabulary. Consider a corpus with a 100,000 vocabulary and 10,000 documents. This yields a matrix with one billion values. If each element is a 64-bit integer (8 bytes), this matrix will require 8GB of memory.\n",
    "\n",
    "To conserve memory, you will not use all the documents to make the word count matrix. Instead, you should create a function/method to create a word count matrix using the first N documents. Do this for values of N = [10,25,50,100]. For each value of N, compute the size of the matrix in MB using the function given below.\n",
    "\n",
    "Additionally, for each value of N convert the matrix to a sparse matrix using the scipy.sparse.csr_matrix function. Compute the size of the sparse matrix in MB using the same function. Summarize the results in a table comparing the memory used for each value of N for the regular matrix and the sparse matrix.\n",
    "\n",
    "def size_of_array_mb(a):\n",
    "    return round(a.data.nbytes*1e-6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88325"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
